<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>CserDuのBlog</title>
  
  <subtitle>岁月无痕,有迹可寻</subtitle>
  <link href="https://cserdu.github.io/atom.xml" rel="self"/>
  
  <link href="https://cserdu.github.io/"/>
  <updated>2023-02-25T15:51:43.364Z</updated>
  <id>https://cserdu.github.io/</id>
  
  <author>
    <name>CserDu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>扩散模型汇总</title>
    <link href="https://cserdu.github.io/2023/02/25/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB/"/>
    <id>https://cserdu.github.io/2023/02/25/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B%E6%B1%87%E6%80%BB/</id>
    <published>2023-02-25T15:50:27.739Z</published>
    <updated>2023-02-25T15:51:43.364Z</updated>
    
    <content type="html"><![CDATA[<h1 id="扩散模型汇总"><a href="#扩散模型汇总" class="headerlink" title="扩散模型汇总"></a>扩散模型汇总</h1><h2 id="一、diffusion-model"><a href="#一、diffusion-model" class="headerlink" title="一、diffusion model"></a>一、diffusion model</h2><h3 id="视角-1"><a href="#视角-1" class="headerlink" title="视角 1"></a>视角 1</h3><p>理解扩散模型最简单的方式是将它看成一个马尔科夫多层变分自编码器，并且具有三个限制：</p><ul><li>隐空间向量的维度和数据维度一致；</li><li>隐空间向量编码器不需要学习，中间每一个向量都是基于前一个向量的高斯分布；</li><li>隐空间向量/高斯分布的参数随着时间变化，最终是一个标准的高斯分布。</li></ul><span id="more"></span><p>从第一个限制出发，后验分布：</p><script type="math/tex; mode=display">q(x_{1:T}|x_0)=\prod_{t=1}^Tq(x_t|x_{t-1})</script><p>式中，$x_0$表示初始数据，$x_{1}…x_T$表示中间隐向量。</p><p>根据第二个限制，$encoder\ transitions$定义为：</p><script type="math/tex; mode=display">q(x_t|x_{t-1})=N(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I)</script><p>即：$x_t=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}z_t$，其中$z_t\sim N(0,I)$. </p><p>那么递推可以得到：$x_t=\sqrt{\overline{\alpha}_t}x_{0}+\sqrt{1-\overline{\alpha}_t}\overline{z}_t$，其中$\overline{z}_t\sim N(0,I)$.</p><p>根据第三个限制：</p><script type="math/tex; mode=display">p(x_{0:T})=p(x_T) \prod_{t=1}^T p_\theta(x_{t-1}|x_t)</script><p>其中，$p(x_T)=N(x_t;0,I)$，$p_\theta(x_{t-1}|x_t)$是逆扩散过程的$transitions$，也就是我们需要学习的网络，$\theta$是参数。</p><p>对于正向扩散过程，并没有可学习的参数，所以我们主要考虑优化逆向过程$p_\theta(x_{t-1}|x_t)$，$\theta$就是需要学习的网络参数。</p><p>和$HVAE$一样，扩散模型的优化目标也是最大化对数似然函数，其推导过程如下：</p><script type="math/tex; mode=display">\begin{align}\log p(x)&=\log \int p(x_{0:T})dx_{1:T}\\&=\log \int \frac{p(x_{0:T})q(x_{1:T}|x_0)}{q(x_{1:T}|x_0)}dx_{1:T}\\&=\log E_{q(x_{1:T}|x_0)}[\frac{p(x_{0:T})}{q(x_{1:T}|x_0)}]\\&\ge E_{q(x_{1:T}|x_0)} [ \log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)}]\\&=E_{q(x_{1:T}|x_0)} [ \log \frac{p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t)}{\prod_{t=1}^Tq(x_t|x_{t-1})}]\\&=E_{q(x_{1:T}|x_0)} [ \log \frac{p(x_T)p_\theta(x_0|x_1) \prod_{t=2}^Tp_\theta(x_{t-1}|x_t)}{ q(x_T|x_{T-1})\prod_{t=1}^{T-1}q(x_t|x_{t-1})}]\\&=E_{q(x_{1:T}|x_0)} [ \log \frac{p(x_T)p_\theta(x_0|x_1) \prod_{t=1}^{T-1}p_\theta(x_{t}|x_{t+1})}{q(x_T|x_{T-1})\prod_{t=1}^{T-1}q(x_t|x_{t-1})}]\\&=E_{q(x_{1:T}|x_0)} [ \log \frac{p(x_T)p_\theta(x_0|x_1)}{q(x_T|x_{T-1})}]+E_{q(x_{1:T}|x_0)} [ \log \prod_{t=1}^{T-1} \frac{p_\theta(x_t|x_{t+1})}{q(x_t|x_{t-1})}]\\&=E_{q(x_{1:T}|x_0)}[\log p_\theta(x_0|x_1)]+E_{q(x_{1:T}|x_0)} [\log \frac{p(x_T)}{q(x_T|x_{T-1})}]+E_{q(x_{1:T}|x_0)} [\sum_{t=1}^{T-1} \frac{p_\theta(x_t|x_{t+1})}{q(x_t|x_{t-1})}]\\&=E_{q(x_1|x_0)}[\log p_\theta(x_0|x_1)]+E_{q(x_{T-1},x_T|x_0)}[\log \frac{p(x_T)}{q(x_T|x_{T-1})}]+\sum_{t=1}^{T-1} E_{q(x_{t-1},x_t,x_{t+1}|x_0)} [\frac{p_\theta(x_t|x_{t+1})}{q(x_t|x_{t-1})}]\\&=\underbrace{E_{q(x_1|x_0)}[\log p_\theta(x_0|x_1)]}_{重构项}-\underbrace{E_{q(x_{T-1}|x_0)}[D_{KL}(q(x_T|x_{T-1})||p(x_T))]}_{先验匹配项}-\sum_{t=1}^{T-1} \underbrace{E_{q(x_{t-1},x_t+1|x_0)}[D_{KL}(q(x_t|x_{t-1})||p_\theta(x_t|x_{t+1}))]}_{一致项}\\&=L_0-L_T-\sum_{i=1}^{T-1}L_i\end{align}</script><p>为了最大化$\log p(x)$，我们需要最大化$L_0$，最小化$L_T$和$L_i$.</p><p>上述所有项都是计算期望值，可以使用蒙特卡洛采样近似计算。但是仍然存在一个问题，对于每个$L_i$，需要采样$x_{t-1}$和$x_{t+1}$来计算$x_t$，方差过大，因此需要改进。主要的改进思路是消除一个量，关键的地方在于：</p><script type="math/tex; mode=display">q(x_t|x_{t-1})=q(x_t|x_{t-1},x_0)</script><p>根据马尔科夫链的性质，式$(16)$显然成立。</p><p>那么根据贝叶斯规则：</p><script type="math/tex; mode=display">q(x_t|x_{t-1},x_0)=\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}</script><p>再回到式$(8)$：</p><script type="math/tex; mode=display">\begin{align}\log p(x)&=E_{q(x_{1:T}|x_0)}[\log \frac{p(x_T)\prod_{t=1}^T p_\theta(x_{t-1}|x_t)}{\prod_{t=1}^T q(x_t|x_{t-1})}]\\&=E_{q(x_{1:T}|x_0)} [\log \frac{p(x_T)p_\theta(x_0|x_1)\prod_{t=2}^Tp_\theta(x_{t-1}|x_t)}{q(x_1|x_0)\prod_{t=2}^Tq(x_t|x_{t-1})}]\\&=E_{q(x_{1:T}|x_0)} [\log \frac{p(x_T)p_\theta(x_0|x_1)}{q(x_1|x_0)}+ \log \prod_{t=2}^T \frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1},x_0)}]\\&=E_{q(x_{1:T}|x_0)} [\log \frac{p(x_T)p_\theta(x_0|x_1)}{q(x_1|x_0)}+\log \prod_{t=2}^T \frac{p_\theta(x_{t-1}|x_t)}{\frac{q(x_{t-1}|x_t,x_0)q(x_t|x_0)}{q(x_{t-1}|x_0)}}]\\&=E_{q(x_{1:T}|x_0)}[\log \frac{p(x_T)p_\theta(x_0|x_1)}{q(x_1|x_0)}+ \log \prod_{t=2}^T (\frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_t,x_0)}\cdot \frac{q(x_{t-1}|x_0)}{q(x_t|x_0)})]\\&=E_{q(x_{1:T}|x_0)}[\log \frac{p(x_T)p_\theta(x_0|x_1)}{q(x_1|x_0)}+\log \frac{q(x_1|x_0)}{q(x_T|x_0)}+ \log \prod_{t=2}^T \frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_t,x_0)}]\\&=E_{q(x_{1:T}|x_0)}[\log \frac{p(x_T)p_\theta(x_0|x_1)}{q(x_T|x_0)}+\sum_{t=2}^T \frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_t,x_0)}]\\&=E_{q(x_1|x_0)}[\log p_\theta(x_0|x_1)]+E_{q(x_T|x_0)}[\log \frac{p(x_T)}{q(x_T|x_0)}]+\sum_{t=2}^T E_{q(x_t,x_{t-1}|x_0)}[\log \frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_t,x_0)}]\\&=\underbrace{E_{q(x_1|x_0)}[\log p_\theta(x_0|x_1)]}_{重构项}-\underbrace{D_{KL}(q(x_T|x_0)||p(x_T))}_{先验匹配项}-\sum_{t=2}^T \underbrace{E_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))]}_{去噪匹配项}\\&=L_0-L_T-\sum_{i=2}^T L_i\end{align}</script><p>注意到，当式$(26)$中$T=1$时，扩散模型与普通的$VAE$模型的优化目标一样。</p><p>根据式$(26)$，扩散模型优化的主要目标就是去噪匹配项。对于每一项$KL$散度$D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))$中的$q(x_{t-1}|x_t,x_0)$，我们可以求出它是一个高斯分布：</p><script type="math/tex; mode=display">\begin{align}q(x_{t-1}|x_t,x_0)&=\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\\&=\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)}{q(x_t|x_0)}\\&=\frac{N(x_t;\sqrt{\alpha_t}x_{t-1},(1-\alpha_t)I) \cdot N(x_{t-1};\sqrt{\overline{\alpha}_{t-1}}x_0,(1-\overline{\alpha}_{t-1})I)}{N(x_t;\sqrt{\overline{\alpha}_t}x_0,(1-\overline{\alpha}_t)I)}\\             &\varpropto e^{-\frac{(x_t-\sqrt{\alpha_t}x_{t-1})^2}{2(1-\alpha_t)}+\frac{(x_{t-1}-\sqrt{\overline{\alpha}_{t-1}}x_0)^2}{2(1-\overline{\alpha}_{t-1})}-\frac{(x_t-\sqrt{\overline{\alpha}_t}x_0)^2}{2(1-\overline{\alpha}_t)}}\\&\varpropto N(x_{t-1};\underbrace{\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_t)x_t+\sqrt{\overline{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\overline{\alpha}_t}}_{\mu_q(x_t,x_0)},\underbrace{\frac{(1-\alpha_t)(1-\overline{\alpha}_{t-1})}{1-\overline{\alpha}_t}I}_{\Sigma_q(t)})\end{align}</script><p>根据式$(32)$，$q(x_{t-1}|x_t,x_0)$是一个均值为$\mu_q(x_t,x_0)$，方差为$\Sigma_q(t)$的高斯分布，其中方差是一个常数。</p><p>所以为了优化$KL$散度，我们也将$p_\theta(x_{t-1}|x_t)$建模为高斯分布，即$p_\theta(x_{t-1}|x_t)=N(x_{t-1};\mu_\theta,\Sigma_q(t))$.</p><p>对于两个一元高斯分布，$KL$散度计算表达式为：</p><script type="math/tex; mode=display">KL(N(\mu_1,\sigma_1^2)||N(\mu_2,\sigma_2^2))=\log \frac{\sigma_2}{\sigma_1}-\frac{1}{2}+\frac{\sigma_1^2+(\mu_1-\mu_2)^2}{2\sigma_2^2}</script><p>对于多元高斯分布，其概率密度函数表达式为：</p><script type="math/tex; mode=display">N(x;\mu,\Sigma)=\frac{1}{(2\pi)^K|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}</script><p>当$K=1$时，对应一元高斯分布概率密度函数。</p><p>两个多元高斯分布的$KL$散度表达式为：</p><script type="math/tex; mode=display">KL(N(x;\mu_1,\Sigma_1)||N(x;\mu_2,\Sigma_2))=\frac{1}{2}[\log \frac{|\Sigma_2|}{|\Sigma_1|}-K+tr(\Sigma_2^{-1}\Sigma_1)+(\mu_1-\mu_2)^T\Sigma_2^{-1}(\mu_1-\mu_2)]</script><p>所以：</p><script type="math/tex; mode=display">\begin{align}&\underset{\theta}{\arg \min}\ D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))\\=&\underset{\theta}{\arg \min}\ D_{KL}(N(x_{t-1};\mu_q,\Sigma_q(t))||N(x_{t-1};\mu_\theta,\Sigma_q(t)))\\=&\underset{\theta}{\arg \min}\ \frac{1}{2}[\log \frac{|\Sigma_q(t)|}{|\Sigma_q(t)|}-d+tr(\Sigma_q(t)^{-1}\Sigma_q(t))+(\mu_q-\mu_\theta)\Sigma_q(t)^{-1}(\mu_q-\mu_\theta)]\\=&\underset{\theta}{\arg \min}\ \frac{1}{2}[\log 1-d+d+(\mu_q-\mu_\theta)\Sigma_q(t)^{-1}(\mu_q-\mu_\theta)]\\=&\underset{\theta}{\arg \min}\ \frac{1}{2}[(\mu_q-\mu_\theta)\Sigma_q(t)^{-1}(\mu_q-\mu_\theta)]\\=&\underset{\theta}{\arg \min}\ \frac{1}{2\Sigma_q(t)}||\mu_q-\mu_\theta||^2_2\end{align}</script><p>在式$(32)$中$\mu_q=\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_t)x_t+\sqrt{\overline{\alpha}_{t-1}}(1-\alpha_t)x_0}{1-\overline{\alpha}_t}$，为了让$\mu_\theta$尽可能接近$\mu_q$，我们也可以将$\mu_\theta$定义成$\mu_q$的形式，但是不能依靠$x_0$，因为网络去噪的目标就是生成$x_0$，所以不知道$x_0$的值。</p><script type="math/tex; mode=display">\mu_\theta(x_t,t)=\frac{\sqrt{\alpha_t}(1-\overline{\alpha}_t)x_t+\sqrt{\overline{\alpha}_{t-1}}(1-\alpha_t)x_\theta(x_t,t)}{1-\overline{\alpha}_t}</script><p>其中，$x_\theta(x_t,t)$就是我们的网络预测的$x_0$.</p><p>带入到式$(42)$中，得到：</p><script type="math/tex; mode=display">\begin{align}&\underset{\theta}{\arg \min}\ D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))\\=&\underset{\theta}{\arg \min}\ \frac{1}{2\Sigma_q(t)}[||\frac{\sqrt{\alpha_t}(1-\alpha_t)}{1-\overline{\alpha}_t}(x_\theta(x_t,t)-x_0)||^2]\end{align}</script><p>综上，扩散模型的目标就是去训练一个神经网络，从任意的噪声图片$x_t$中去预测$x_0$.</p><script type="math/tex; mode=display">\underset{\theta}{\arg \min}\ E_{t\sim U(2,T)}(E_{q(x_t|x_0)}[D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))])</script><h3 id="视角-2"><a href="#视角-2" class="headerlink" title="视角 2"></a>视角 2</h3><p>上述从$x_t$预测$x_0$是一种理解，还可以从预测噪声$\epsilon_\theta(x_t,t)$的角度来理解。</p><p>根据$x_t=\sqrt{\overline{\alpha}_t}x_{0}+\sqrt{1-\overline{\alpha}_t}\overline{z}_t$，可以得到：</p><script type="math/tex; mode=display">x_0=\frac{x_t-\sqrt{1-\overline{\alpha}_t}\overline{z}_t}{\sqrt{\overline{\alpha}_t}}</script><p>将上式带入到式$(32)$中的$\mu_q(x_t,x_0)$，得到：</p><script type="math/tex; mode=display">\mu_q(x_t)=\frac{1}{\sqrt{\alpha_t}}x_t-\frac{1-\alpha_t}{\sqrt{(1-\overline{\alpha}_t)\alpha_t}}\overline{z}_t</script><p>式$(47)$与式$(32)$的区别在于将$x_0$替换成了$\overline{z}_t$（这里的$\overline{z}_t$表示从$x_{t-1}$到$x_t$添加的高斯噪声），那么同理，最终的优化目标就变成了：</p><script type="math/tex; mode=display">\underset{\theta}{\arg \min}\ \frac{1}{2\Sigma_q(t)}\frac{(1-\alpha_t)^2}{(1-\overline{\alpha}_t)\alpha_t}[||\overline{z}_t-\epsilon_\theta(x_t,t)||^2_2]</script><h3 id="视角-3"><a href="#视角-3" class="headerlink" title="视角 3"></a>视角 3</h3><p>从$score\ function$的角度去理解扩散模型。</p><p>首先需要知道$Tweedie’s\ Formula$：对于一个高斯变量$z\sim N(z;\mu_z,\Sigma_z)$，有：</p><script type="math/tex; mode=display">E\ [\mu_z|z]=z+\Sigma_z\nabla_z \log p(z)</script><p>根据$x_t=\sqrt{\overline{\alpha}_t}x_{0}+\sqrt{1-\overline{\alpha}_t}\overline{z}_t$，有：</p><script type="math/tex; mode=display">E\ [\mu_{x_t}|x_t]=x_t+(1-\overline{\alpha}_t)\nabla_{x_t} \log p(x_t)</script><p>$E\ [\mu_{x_t|x_t}]$表示对$x_t$的均值估计，那么理想的$groud\ truth$就是$\sqrt{\overline{\alpha}_t}x_0$.</p><p>所以：</p><script type="math/tex; mode=display">\begin{align}\sqrt{\overline{\alpha}_t}x_0=x_t+(1-\overline{\alpha}_t)\nabla_{x_t} \log p(x_t)\\x_0=\frac{x_t+(1-\overline{\alpha}_t)\nabla_{x_t} \log p(x_t)}{\sqrt{\overline{\alpha}_t}}\end{align}</script><p>同样的道理，我们把式$(52)$带入式$(32)$中的$\mu_q(x_t,x_0)$，得到：</p><script type="math/tex; mode=display">\mu_q(x_t)=\frac{1}{\sqrt{\alpha_t}}x_t+\frac{1-\alpha_t}{\sqrt{\alpha_t}}\nabla_{x_t} \log p(x_t)</script><p>与式$(32)$相比，把$x_0$替换成了$\nabla_{x_t} \log p(x_t)$.</p><p>最终的优化目标就是：</p><script type="math/tex; mode=display">\underset{\theta}{\arg \min}\ \frac{1}{2\Sigma_q(t)}\frac{(1-\alpha_t)^2}{\alpha_t}[||s_\theta(x_t,t)-\nabla_{x_t}\log p(x_t)||^2]</script><p>如何理解这个$\nabla_{x_t}\log p(x_t)$呢？（注意这里的$p$不是$p_\theta$，它表示$x_t$的概率密度函数）</p><p>本质上，这个$score \ function$是概率密度函数的对数求导，不考虑对数，那就是概率密度函数的导数。考虑高斯分布，均值左侧的点的导数都大于0，均值右侧的点的导数都小于0，而在均值处导数为0. 很显然，这个导数值反映的是各个点朝着均值处的方向，也就意味着似然对数最大的方向。并且，如果两个概率密度函数的导数相等，那么这两个概率分布只相差一个常数，即上下平移得到，又因为概率分布的积分等于1，所以这两个概率分布只能相同！这也就意味着，让网络去拟合$score\ function$，如果训练的足够好，那么这个网络就与数据分布相同。而这一项是从2到$T$的求和，那也就表明从2到$T$拟合的数据分布与原数据相同，那么去噪过程自然也就可以还原出原图了。</p><p>其实，$score function$与另一类模型——$ score \ based\ generative \ model$关联密切。对于任意的概率密度函数，我们可以写成一种通用的形式：</p><script type="math/tex; mode=display">p_\theta(x)=\frac{1}{Z_\theta}e^{-f_\theta(x)}</script><p>其中$Z_\theta$是归一化常数，$Z_\theta=\int e^{-f_\theta(x)}dx$.</p><p>通常情况下，$Z_\theta$是很难求解的一个量。为了方便利用式$(55)$，使用$\nabla_x \log p_\theta(x)$：</p><script type="math/tex; mode=display">\begin{align}\nabla_x \log p_\theta(x)&=\nabla_x \log (\frac{1}{Z_\theta}e^{-f_\theta(x)})\\&=-\nabla_x \log f_\theta(x)\end{align}</script><p>可以看到，通过求导，消除了归一化常数$Z_\theta$. 所以在这里模型中，通常用神经网络去拟合$\nabla_x\log p_\theta(x)$，这与直接拟合$p_\theta(x)$效果是等价的，最终都会使得网络学习到的数据分布与原数据分布相同。</p><h3 id="视角-4"><a href="#视角-4" class="headerlink" title="视角 4"></a>视角 4</h3><p>从$SDE$的角度去理解扩散模型，在此之前，先了解一下布朗运动、伊藤积分以及随机微分方程（$SDE$）的一些知识。</p><p>我们假设在时刻$t$，花粉的位置为$B_t$，那么在一个很小的时间段$\Delta t$内，其位置的增量是一个均值为0的正态分布：</p><script type="math/tex; mode=display">B_{t+\Delta t}=B_t+\alpha*N(0,\Delta t)</script><p>如果$\alpha=1$，那么就是标准的布朗运动，我们主要研究的就是它，即：</p><script type="math/tex; mode=display">B_{t+\Delta t}=B_t+N(0,\Delta t)</script><p>标准布朗运动有个很重要的性质：连续但是几乎处处不可微分。当$\Delta t \to 0$时，$B_t$是连续的，但是却无法对其进行微分，因为每一个增量都来自一个正态分布。</p><p>因为$B_t$无法求微分，所以如果在一个微分方程中含有类似$B_t$这样的布朗运动，那么就无法使用常微分方程（$ODE$）的方法求解。因此需要定义一个新的积分，伊藤积分：</p><script type="math/tex; mode=display">\int_0^t g(X_s,s)dB_s</script><p>关于$SDE$，通常写成微分的形式：</p><script type="math/tex; mode=display">dX_t=f(t,X_t)dt+\sigma(t,X_t)dW_t</script><p>其中，$f(\cdot)$被称作漂移项，$\sigma(\cdot)$被称作扩散项，$dW_t$就是布朗运动的微小增量。</p><p>接下来从$SDE$的角度来介绍扩散模型：</p><p>前向过程是一个高斯扩散过程，现在我们将这个过程连续化，考虑$\Delta t$时间内的扩散，那么$\beta_t$在时间上也是连续的，令$\beta_t=\beta(t)\Delta t$.那么：</p><script type="math/tex; mode=display">\begin{align}x_t&=\sqrt{1-\beta_t}x_{t-1}+\sqrt{\beta_t}N(0,I)\\&=\sqrt{1-\beta(t)\Delta t}x_{t-1}+\sqrt{\beta(t)\Delta t}N(0,I)\\\end{align}</script><p>当$\Delta t \to 0$时，对$(63)$进行一阶泰勒展开，则：</p><script type="math/tex; mode=display">\begin{align}x_t&\approx (1-\frac{\beta(t)\Delta t}{2})x_{t-1}+\sqrt{\beta(t)\Delta t}N(0,I)\\&=x_{t-1}-\frac{\beta(t)\Delta t}{2}x_{t-1}+\sqrt{\beta(t)\Delta t}N(0,I)\end{align}</script><p>所以：</p><script type="math/tex; mode=display">dx_t=-\frac{1}{2}\beta(t)x_tdt+\sqrt{\beta(t)}dw_t</script><p>比较$(61)$与$(61)$，可以发现扩散过程就是$SDE$过程，并且：</p><script type="math/tex; mode=display">f(x_t,t)=-\frac{1}{2}\beta(t)x_t\\\sigma(x_t,t)=\sqrt{\beta(t)}</script><p>既然前向过程是一个$SDE$过程，那么根据已有结论，一个$SDE$的逆向过程也是一个$SDE$，并且逆向$SDE$的表达式为：</p><script type="math/tex; mode=display">dx=[f(x,t)-g(t)^2\nabla_x \log p_t(x)]dt+g(t)d\overline{w}</script><p>上式中$g(t)$对应$\sigma(t,x_t)$.</p><p>带入到扩散模型中，得到：</p><script type="math/tex; mode=display">dx_t=-\frac{1}{2}\beta(t)[x_t+2\nabla_{x_t}log q_t(x_t)]dt+\sqrt{\beta(t)}d\overline{w_t}</script><p>并且，已经证明，每个扩散过程都有对应的确定性的$ODE$形式，表达式为：</p><script type="math/tex; mode=display">dx=[f(x,t)-\frac{1}{2}g(t)^2\nabla_x \log p_t(x)]dt</script><p>带入到扩散模型中，得到：</p><script type="math/tex; mode=display">dx_t=-\frac{1}{2}\beta(x)[x_t+\nabla_{x_t}\log q_t(x_t)]dt</script><p>那么就可以使用$SDE/ODE\ solver$来求解。</p><h2 id="二、DDIM"><a href="#二、DDIM" class="headerlink" title="二、DDIM"></a>二、DDIM</h2><h2 id="三、classifier-guidance"><a href="#三、classifier-guidance" class="headerlink" title="三、classifier guidance"></a>三、classifier guidance</h2><p>在逆扩散过程中，加入分类条件$y$时，使用下面的方式采样：</p><script type="math/tex; mode=display">\begin{align}p(x_t|x_{t+1},y)&=\frac{p(x_t,x_{t+1},y)}{p(x_{t+1},y)}\\&=\frac{p(x_t,x_{t+1},y)}{p(y|x_{t+1})p(x_{t+1})}\\&=\frac{p(x_{t+1})p(x_t|x_{t+1})p(y|x_t,x_{t+1})}{p(y|x_{t+1})p(x_{t+1})}\\&=\frac{p(x_t|x_{t+1})p(y|x_t,x_{t+1})}{p(y|x_{t+1})}\\&=\frac{p(x_t|x_{t+1})p(y|x_t)}{p(y|x_{t+1})}\end{align}</script><p>从$(75)$到$(76)$是因为：</p><p>在上述的逆扩散过程中，我们的目的是<strong>在分类条件$y$的引导下，从$x_{t+1}$去噪得到$x_t$</strong>. 这句话的深层含义是：<strong>对于当前时刻，$x_{t+1}$是已知的，类别引导信息$y$只会影响$x_t$，所以此时$y$与$x_{t+1}$是无关/独立的</strong>，那么：</p><script type="math/tex; mode=display">\begin{align}p(y|x_t,x_{t+1})&=p(x_{t+1}|x_t,y)\frac{p(y|x_t)}{p(x_{t+1}|x_t)}\\&=p(x_{t+1}|x_t)\frac{p(y|x_t)}{p(x_{t+1}|x_t)}\\&=p(y|x_t)\end{align}</script><p>还需理解的一点是，这个类引导条件$y$到底有什么具体含义或者性质。例如我们现在想要从噪声中生成猫的图片，该怎么添加这个引导信息呢？</p><p>如果$y$代表猫这个类别，那么$p(y)$就表示猫的概率分布，这个概率分布怎么来的？数据集！！！在我们的数据集中，有各种类别的图片，每种类别都有自己的概率分布。因此，当我们给网络提供猫这个类别的引导信息后，去噪过程就会倾向于生成猫这个类别的概率分布。所以，<strong>$p(y)$不仅与$p(x_{t+1})$独立/无关，它还是个常量</strong>！！！</p><p>再回到$(76)$，$p(y|x_{t+1})$其实就是$p(y)$，是个常数，所以：</p><script type="math/tex; mode=display">p_{\theta,\phi}(x_t|x_{t+1},y)=Z\cdot p_\theta(x_t|x_{t+1})\cdot p_\phi(y|x_t)</script><p>其中，$Z$是标准化常数。式$(80)$很难求解，可以用高斯分布近似。</p><p><strong>对于$p_\theta(x_t|x_{t+1})$项：</strong>对应无条件引导的逆扩散过程，因此：$p_\theta(x_t|x_{t+1})=N(\mu,\sigma^2)=\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(x_t-\mu)^2}{2\sigma^2}}$. 那么：</p><script type="math/tex; mode=display">\log p_\theta(x_t|x_{t+1})=-\frac{1}{2\sigma^2}(x_t-\mu)^2+C</script><p><strong>对于$p_\phi(y|x_t)$项：</strong>我们对$\log p_\phi(y|x_t)$在$x_t=\mu$处进行一阶泰勒展开：</p><script type="math/tex; mode=display">\begin{align}\log p_\phi(y|x_t)&\approx \log p_\phi(y|x_t)|_{x_t=\mu}+(x_t-\mu)\nabla_{x_t}\log p_\phi(y|x_t)|_{x_t=\mu}\\&=(x_t-\mu)g+C_1\end{align}</script><p>其中，$g=\nabla_{x_t}\log p_\phi(y|x_t)|_{x_t=\mu}$ .</p><p>综上所述：</p><script type="math/tex; mode=display">\begin{align}\log(p_\theta(x_t|x_{t+1})p_\phi(y|x_t))&\approx-\frac{1}{2\sigma^2}(x_t-\mu)^2+(x_t-\mu)g+C_2\\&=-\frac{1}{2\sigma^2}(x_t-\mu-\sigma^2g)^2+\frac{1}{2}\sigma^2g^2+C_2\\&=-\frac{1}{2\sigma^2}(x_t-\mu-\sigma^2g)+C_3\\&=\log p(z)+C_4\end{align}</script><p>其中，$z\sim N(\mu+\sigma^2g,\sigma^2),g=\nabla_{x_t}\log p_\phi(y|x_t)|_{x_t=\mu}$. </p><p>由上式可以看出，类条件引导的逆扩散过程与无条件类似，都可以用高斯分布近似，但是均值需要加上偏移量$\sigma^2g$ . 算法如下图所示：</p><p><img src="D:\research\AIGC\扩散模型\扩散模型汇总.assets\image-20230130172919805.png" alt="image-20230130172919805"></p><p>​                                                                    图1 类条件引导的逆扩散采样算法</p><p>此外，如何去理解$g=\nabla_{x_t}p_\phi(y|x_t)|_{x_t=\mu}$？</p><p><img src="D:\research\AIGC\扩散模型\扩散模型汇总.assets\image-20230130181014944.png" alt="image-20230130181014944"></p><p>​                                                                                                图2 计算$g$的代码</p><p>从这段代码可以看出，实际上$g$值与交叉熵损失函数只是相差了一个系数和负号，因为：</p><p><img src="D:\research\AIGC\扩散模型\扩散模型汇总.assets\image-20230130181223954.png" alt="image-20230130181223954"></p><p>​                                                                                图3 直接计算$g$与使用交叉熵损失函数计算对比</p><p>结果为：</p><p><img src="D:\research\AIGC\扩散模型\扩散模型汇总.assets\image-20230130181310697.png" alt="image-20230130181310697"></p><p>​                                                                                    图4 对比结果</p><p>交叉熵损失函数取了均值，所以相差2倍和一个负号。</p><p>从这里可以看出，$\log p_\phi(y|x_t)$衡量了$x_t$与$y$的距离，两者的关系如下：</p><script type="math/tex; mode=display">\log p_\phi(y|x_t)=-N \cdot CrossEntroty(x_t,y)</script><p>所以带条件的$\mu$更新本质上就是使用$SGD$梯度下降法：</p><script type="math/tex; mode=display">\begin{align}\mu_{t-1}&=\mu_t+s\sigma^2\cdot \nabla_{x_t}\log p_\phi(y|x_t)\\&=\mu_t-\alpha\nabla_\phi(x_t,y)\end{align}</script><h2 id="四、classifier-free-guidance"><a href="#四、classifier-free-guidance" class="headerlink" title="四、classifier-free guidance"></a>四、classifier-free guidance</h2>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;扩散模型汇总&quot;&gt;&lt;a href=&quot;#扩散模型汇总&quot; class=&quot;headerlink&quot; title=&quot;扩散模型汇总&quot;&gt;&lt;/a&gt;扩散模型汇总&lt;/h1&gt;&lt;h2 id=&quot;一、diffusion-model&quot;&gt;&lt;a href=&quot;#一、diffusion-model&quot; class=&quot;headerlink&quot; title=&quot;一、diffusion model&quot;&gt;&lt;/a&gt;一、diffusion model&lt;/h2&gt;&lt;h3 id=&quot;视角-1&quot;&gt;&lt;a href=&quot;#视角-1&quot; class=&quot;headerlink&quot; title=&quot;视角 1&quot;&gt;&lt;/a&gt;视角 1&lt;/h3&gt;&lt;p&gt;理解扩散模型最简单的方式是将它看成一个马尔科夫多层变分自编码器，并且具有三个限制：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;隐空间向量的维度和数据维度一致；&lt;/li&gt;
&lt;li&gt;隐空间向量编码器不需要学习，中间每一个向量都是基于前一个向量的高斯分布；&lt;/li&gt;
&lt;li&gt;隐空间向量/高斯分布的参数随着时间变化，最终是一个标准的高斯分布。&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="扩散模型" scheme="https://cserdu.github.io/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"/>
    
    <category term="深度学习" scheme="https://cserdu.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="Python" scheme="https://cserdu.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>测试文章</title>
    <link href="https://cserdu.github.io/2023/02/25/text/"/>
    <id>https://cserdu.github.io/2023/02/25/text/</id>
    <published>2023-02-25T14:15:18.173Z</published>
    <updated>2023-02-25T14:23:18.712Z</updated>
    
    <content type="html"><![CDATA[<p>这确实应该是每个开发人员都应该牢记的规则，而不只是个空头文字。</p><span id="more"></span><p>我再次强调一遍：如果你无法为合并请求提供合理的解释或者说明，那么请不要提交。真的就这么简单。在不解释为什么存在这些合并的情况下，绝对没有合并的借口。</p><p>在这种情况下，我真的认为如果合并请求都没有注释，那么合并还有什么意义！如果你觉得有理由的话，请说出来！让合并提交看起来更加合理。</p><p>因为现在它看起来完全没有意义。我真的厌恶毫无意义的合并。它们只会让历史看起来更糟，更难读懂。<br>————————————————<br>版权声明：本文为CSDN博主「CSDN资讯」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/csdnnews/article/details/129187745">https://blog.csdn.net/csdnnews/article/details/129187745</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这确实应该是每个开发人员都应该牢记的规则，而不只是个空头文字。&lt;/p&gt;</summary>
    
    
    
    <category term="Python" scheme="https://cserdu.github.io/categories/Python/"/>
    
    
    <category term="Anaconda" scheme="https://cserdu.github.io/tags/Anaconda/"/>
    
    <category term="Jupyter" scheme="https://cserdu.github.io/tags/Jupyter/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://cserdu.github.io/2023/02/25/hello-world/"/>
    <id>https://cserdu.github.io/2023/02/25/hello-world/</id>
    <published>2023-02-25T11:27:56.339Z</published>
    <updated>2023-02-25T15:53:09.668Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for <span id="more"></span> more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
    
    
    
    <category term="漫谈" scheme="https://cserdu.github.io/categories/%E6%BC%AB%E8%B0%88/"/>
    
    
  </entry>
  
</feed>
